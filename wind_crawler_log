
您已达到允许的最大指标提取数，请稍后再使用！

sample cmds:
  sudo python runcrawler.py offline "Daily Market Spider"
  export CRAWLER_ENV=release
  echo "2017 1 13 2017 1 13" > /tmp/spider.ranges


to modify charts:
  http://localhost:9080/views/test/charts.html#33

db.charts.update({"_id" : 419}, {$rename : {"dataSource.parameters.:reportDate" : "dataSource.parameters.:endDate"}}, false, true)
db.charts.update({"_id" : 419}, {$rename : {"dataSource.parameters.:compareDate" : "dataSource.parameters.:startDate"}}, false, true)

419 风格指数表现1W, 32 风格指数表现YTD在6.6上的已经可以动态变化了, 11月4日前几天有数据, 缺失的我晚些再导进来

33  A股行业表现1W

add:
        'extraParams': 'Days=Alldays;Fill=Previous',
to config.py for daily_market spider temporarily

$ cat /tmp/spider.ranges
2 016 1 1 2016 1 1

echo "2016 11 28 2017 1 14" > /tmp/spider.ranges


25  上证综指一周表现

sudo python runcrawler.py offline "Wsi Spider"

2016-12-16: 1481817600
2016-12-17: 1481904000
2017-01-15: 1484409600

db.wind_wsi.find({date : {$gte : 1481817600000, $lte : 1484409600000}, "windcode":{"$in":["000001.SH","000903.SH","000904.SH","000905.SH","000906.SH","000300.SH","399001.SZ","399005.SZ","399006.SZ"]}}).count()
4837
  000001.SH [2016-12-16 => 2017-01-13]

[2016-12-16 => 2017-01-15]
db.wind_wsi.find({date : {$gte : 1481817600000, $lte : 1484409600000}, "windcode":{"$in":["000001.SH","000903.SH","000904.SH","000905.SH","000906.SH","000300.SH","399001.SZ","399005.SZ","399006.SZ"]}}).count()
43533
[2016-12-16 => 2017-01-14]
db.wind_wsi.find({date : {$gte : 1481817600000, $lte : 1484323200000}}).count()
43533

/tmp/spider.ranges -> 2016 12 16 2017 1 14
sudo python runcrawler.py offline "Wsi Spider"
[2016-12-16, 2017-1-15]
$ mongoexport -h 10.12.6.6 -d apes -c wind_wsi -f "close,date,windcode" -q '{date : {$gte : 1481817600000, $lte : 1484409600000}, "windcode":{"$in":["000001.SH","000903.SH","000904.SH","000905.SH","000906.SH","000300.SH","399001.SZ","399005.SZ","399006.SZ"]}}' -o ~/tmp/wind_wsi.dat
connected to: 10.12.6.6
exported 43544 records

echo "2016 11 28 2017 1 14" > /tmp/spider.ranges
[2016-11-28, 2017-01-15]
$ mongoexport -h 10.12.6.6 -d apes -c wind_wsd -f "close,date,windcode" -q '{date : {$gte : 1480262400000, $lte : 1484409600000}, "windcode":{"$in":["000001.SH","000903.SH","000904.SH","000905.SH","000906.SH","000300.SH","399001.SZ","399005.SZ","399006.SZ"]}}' -o ~/tmp/wind_wsd.dat
connected to: 10.12.6.6
exported 414 records

if nothing is updated: db.last_fail_point.remove({})

make sure data is valid:
[2016-11-28, 2017-01-15]
db.wind_wsd.find({date : {$gte : 1480262400000, $lte : 1484409600000}, "windcode":{"$in":['000001.SH']}}, {"date" : 1, "close" : 1})
{ "_id" : ObjectId("583e390dc3f6af591d8401c4"), "date" : NumberLong("1480262400000"), "close" : 3277 }
{ "_id" : ObjectId("583e390dc3f6af591d8401c5"), "date" : NumberLong("1480348800000"), "close" : 3282.924 }
{ "_id" : ObjectId("584a1f7f2ff148e73f467440"), "date" : NumberLong("1481126400000"), "close" : 3215.366 }
{ "_id" : ObjectId("584ad51c2ff148e73f4680b5"), "date" : NumberLong("1481212800000"), "close" : 3232.883 }
{ "_id" : ObjectId("584f65972ff148e73f469860"), "date" : NumberLong("1481472000000"), "close" : 3152.97 }
{ "_id" : ObjectId("585293292ff148e73f46b390"), "date" : NumberLong("1481644800000"), "close" : 3140.531 }
{ "_id" : ObjectId("5852be122ff148e73f46b3c3"), "date" : NumberLong("1481731200000"), "close" : 3117.677 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473ae"), "date" : NumberLong("1480435200000"), "close" : 3250.034 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473af"), "date" : NumberLong("1480521600000"), "close" : 3273.309 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b0"), "date" : NumberLong("1480608000000"), "close" : 3243.843 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b1"), "date" : NumberLong("1480694400000"), "close" : 3243.843 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b2"), "date" : NumberLong("1480780800000"), "close" : 3243.843 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b3"), "date" : NumberLong("1480867200000"), "close" : 3204.709 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b4"), "date" : NumberLong("1480953600000"), "close" : 3199.647 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b5"), "date" : NumberLong("1481040000000"), "close" : 3222.242 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b6"), "date" : NumberLong("1481299200000"), "close" : 3232.883 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b7"), "date" : NumberLong("1481385600000"), "close" : 3232.883 }
{ "_id" : ObjectId("5878b8b7878544f6a1e473b8"), "date" : NumberLong("1481558400000"), "close" : 3155.037 }
{ "_id" : ObjectId("5878b8b8878544f6a1e473b9"), "date" : NumberLong("1481817600000"), "close" : 3122.982 }
{ "_id" : ObjectId("5878b8b8878544f6a1e473ba"), "date" : NumberLong("1481904000000"), "close" : 3122.982 }

also copy test machine's [2016-10-14, 2016-11-6] wind_wsd data to online machine
$ mongoexport -h 10.12.6.6 -d apes -c wind_wsd -f "close,date,windcode" -q '{date : {$gte : 1476374400000, $lte : 1478361600000}, "windcode":{"$in":["000001.SH","000903.SH","000904.SH","000905.SH","000906.SH","000300.SH","399001.SZ","399005.SZ","399006.SZ"]}}' -o ~/tmp/wind_wsd_earlier.dat
connected to: 10.12.6.6
exported 153 records

also copy test machine's [2016-10-14, 2016-11-6] wind_wsi data to online machine
$ mongoexport -h 10.12.6.6 -d apes -c wind_wsi -f "close,date,windcode" -q '{date : {$gte : 1476374400000, $lte : 1478361600000}, "windcode":{"$in":["000001.SH","000903.SH","000904.SH","000905.SH","000906.SH","000300.SH","399001.SZ","399005.SZ","399006.SZ"]}}' -o ~/tmp/wind_wsi_earlier.dat
connected to: 10.12.6.6
exported 11616 records

scp /home/kidd/tmp/wind_wsd.dat ahye@120.26.95.121:/home/ahye/
scp /home/kidd/tmp/wind_wsd_earlier.dat ahye@120.26.95.121:/home/ahye/
scp /home/kidd/tmp/wind_wsi.dat ahye@120.26.95.121:/home/ahye/
scp /home/kidd/tmp/wind_wsi_earlier.dat ahye@120.26.95.121:/home/ahye/

风格指数表现:
  2016-10-24 => 2017-01-12

MongoDB shell version: 3.2.10
connecting to: 10.51.3.146:3017/test
Server has startup warnings: 
2017-01-03T14:08:23.772+0800 I CONTROL  [initandlisten] ** WARNING: You are running this process as the root user, which is not recommended.
2017-01-03T14:08:23.772+0800 I CONTROL  [initandlisten] 
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] 
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is 'always'.
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] 
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/defrag is 'always'.
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] 
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. rlimits set to 15229 processes, 65535 files. Number of processes should be at least 32767.5 : 0.5 times number of files.
2017-01-03T14:08:23.773+0800 I CONTROL  [initandlisten] 
> use apes
switched to db apes
> db.wind_wsi.find({date : {$gte : 1476374400000, $lte : 1478361600000}, "windcode":{"$in":['000001.SH',"000903.SH","000904.SH","000905.SH","000906.SH","000300.SH","399001.SZ","399005.SZ","399006.SZ"]}}).count()
11616

风格指数已有数据:
  [2016-10-24, 2017-01-12]

上证综指已有数据:
  [2016-10-24, 2016-11-06]
  [2016-12-16, 2017-1-13]


---------- 2017-03-06 14:40:41
考虑到安全因素,线上linux服务器通过jumper访问:　(如果访问受限请联系max开通权限)
jumper: ssh <your_id>@120.26.195.205 -p 9527
登录jumper后可以进一步通过如下命令登录相应linux服务器 (需要让max帮忙开通以下两台机器的sudo权限)
  nb-ssh bj-tz8-db001
   (120.26.245.253) python策略脚本机器 (启动方式:　http://10.12.6.6:9000/job/apes-crawler-production/　立即构建)　/niub/bj/apes-crawler/apes-crawler-production
  nb-ssh bj-tz8-demo
   (120.26.95.121)　node server /niub/bj/app/wsservice 手动拷贝后到机器上手动运行

手动拷贝方式:
本地到jumper: scp -P 9527 ../apes-crawler.tgz ahye@120.26.195.205:~/
jumper到目标机器:　scp -i /etc/nbrsa/id_rsa <<file>> 10.51.3.146:~/

---------- 2017-03-06 14:22:14
jenkins:
/var/lib/jenkins/workspace/apes-crawler-production

dir=/tmp
echo "creating .tar.gz file ..."
rm -f $dir/crawler.tar.gz.old
if [ -f "$dir/crawler.tar.gz" ]; then
  mv "$dir/crawler.tar.gz" "$dir/crawler.tar.gz.old"
fi
tar zcvf $dir/crawler.tar.gz ../apes-crawler-production
echo "upload tar.gz..."
scp $dir/crawler.tar.gz jenkins@120.26.245.253:/home/jenkins/crawler.tar.gz.tmp
ssh -T jenkins@120.26.245.253 << EOF
  sudo -s
  cp /home/jenkins/crawler.tar.gz.tmp /niub/bj/apes-crawler/crawler.tar.gz.tmp
    rm -f /niub/bj/apes-crawler/crawler.tar.gz.old
    if [ -f "/niub/bj/apes-crawler/crawler.tar.gz" ]; then
      mv "/niub/bj/apes-crawler/crawler.tar.gz" "/niub/bj/apes-crawler/crawler.tar.gz.old"
    fi
    mv "/niub/bj/apes-crawler/crawler.tar.gz.tmp" "/niub/bj/apes-crawler/crawler.tar.gz"
    rm -rf /niub/bj/apes-crawler/apes-crawler
    tar xzf /niub/bj/apes-crawler/crawler.tar.gz -C /niub/bj/apes-crawler/
    export CRAWLER_ENV=release
    # to start the crawler here
    cd /niub/bj/apes-crawler/apes-crawler-production/
    python runcrawler.py restart
EOF
echo "done"

------- 2017-03-06 12:31:20
$ sudo python runcrawler.py offline "EDB Data Spider"
[sudo] password for kidd: 
['anhye@abcft.com']
2017-03-06 12:30:03,103 - 24579 - INFO - connect websocket
2017-03-06 12:30:03,180 - 24579 - INFO - connect finished
10.168.152.131: M0053706
1980-01-01->1989-12-31: M0053706
2017-03-06 12:30:09,556 - 24579 - INFO - response process succeed: EDB Data Spider
1990-01-01->1999-12-31: M0053706
2017-03-06 12:30:10,029 - 24579 - INFO - response process succeed: EDB Data Spider
2000-01-01->2017-03-06: M0053706
2017-03-06 12:30:10,434 - 24579 - INFO - response process succeed: EDB Data Spider
2017-03-06 12:30:12,436 - 24579 - INFO - close websocket
run offline spider "EDB Data Spider" finished

M0053706 is not supported in 120.26.169.235

------------ 2017-03-06 10:44:22
[2017-03-06 10:43:21.646] [INFO] app - cmd: {"cmd":"edb","agent":"10.171.177.232","param":["M0053706","1980-01-01","1989-12-31","Fill=Previous"],"id":1,"quota_usage":1089751,"type":"cmd-response","data":"{\"codeList\":[\"ErrorReport\"],\"fieldList\":[\"OUTMESSAGE\"],\"timeList\":[\"2017-03-06T10:43:20\"],\"data\":[\"Unsupportted wind_codes in free edition.\"],\"errorCode\":-40522005}"}


---------- 2017-03-03 13:39:30
> db.temp_edb_measures.find().count()
46311
> db.temp_edb_measures_p1.find({id : 'M0269492'})
{ "_id" : ObjectId("58b803cc90410fd977d2afb2"), "datasetId" : "wind_edb", "id" : "M0269492", "name" : "全国国有企业:利润总额:社会服务业", "parent" : null, "dataType" : 4, "type" : 2 }
> db.temp_edb_measures_p1.find({_id : {$lt : ObjectId("58b803cc90410fd977d2afb2")}}).count()
791



---------- 2017-03-02 19:58:59
sourcedb.get_temp_edb_data_count()

$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-03-02 19:58:42,951 - 29145 - INFO - connect websocket
2017-03-02 19:58:43,016 - 29145 - INFO - connect finished
total count: 1603
2017-03-02 19:58:43,022 - 29145 - INFO - close websocket
run offline spider "EDB Data Spider" finished


----------- 2017-03-01 14:57:16
-40521009 数据解码失败

--------------- 2017-03-01 11:47:12
$ file /home/kidd/abc/edb/uiautomation/automation_measures/180/edb_measures_180_房地产开发投资完成额累计值.txt
/home/kidd/abc/edb/uiautomation/automation_measures/180/edb_measures_180_房地产开发投资完成额累计值.txt: UTF-8 Unicode (with BOM) text, with CRLF line terminators

----------- 2017-02-28 18:08:35
[root@bj-tz8-db001 apes-crawler]# python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-28 18:08:15,084 - 25887 - INFO - connect websocket
2017-02-28 18:08:15,087 - 25887 - INFO - connect finished
total count: 249471
  -> 249717

------------- 2017-02-27 14:56:37
{ "datasetId" : "wind_edb", "targets" : [ { "id" : "M0043761", "name" : "固定利率国债到期收益率:3个月", "dataType" : 4, "filters" : [ ], "parameters" : [ ] }, { "id" : "M0043764", "name" : "固定利率国债到期收益率:6个月", "dataType" : 4, "filters" : [ ], "parameters" : [ ] }, { "id" : "M0043758", "name" : "固定利率国债到期收益率:1年", "dataType" : 4, "filters" : [ ], "parameters" : [ ] }, { "id" : "M0043762", "name" : "固定利率国债到期收益率:3年", "dataType" : 4, "filters" : [ ], "parameters" : [ ] }, { "id" : "M0043763", "name" : "固定利率国债到期收益率:5年", "dataType" : 4, "filters" : [ ], "parameters" : [ ] }, { "id" : "M0057945", "name" : "固定利率国债到期收益率:6年", "dataType" : 4, "filters" : [ ], "parameters" : [ ] }, { "id" : "M0043757", "name" : "固定利率国债到期收益率:10年", "dataType" : 4, "filters" : [ ], "parameters" : [ ] } ], "appendix" : { "postFilters" : [ ], "sort" : [ ] }, "time" : { "id" : "date", "name" : "时间设置", "dataType" : 1, "filters" : [ { "＄timerange" : { "date" : { "＄gte" : NumberLong("1010073600000"), "＄lt" : NumberLong("1487347200000") } } } ] } } } ], 

--------- 2017-02-27 11:40:44
add more edb measures:
db.measures.find({datasetId : 'wind_edb'}).count()
775

~/tmp
$ mongoexport -h 10.12.6.6 --port 27017 -d apes -c measures -o ~/measures_0227.dat
connected to: 10.12.6.6:27017
exported 5750 records

python /home/kidd/workspace/abc/apes-crawler/tools/import_edb_measurements.py
date without frequency
190

db.measures.find({datasetId : 'wind_edb'}).count()
956


----------- 2017-02-24 11:19:20
echo "2017 2 3 2017 2 21" > /tmp/spider.ranges
export CRAWLER_ENV=release
python runcrawler.py offline "Daily Analysis Spider"

121.199.19.105 (10.132.17.144)
  daily analysis: 400w+
  continue with 121.199.19.218 (10.132.19.39):

2017-02-24 12:26:40,051 - 20266 - INFO - need to upsert daily analysis for 603167.SH
2017-02-24 12:26:40,052 - 20266 - INFO - crawl finished: 5677 for code 603167.SH
2017-02-24 12:26:40,052 - 20266 - INFO - send websocket request: 5678
2017-02-24 12:27:46,019 - 20266 - INFO - errorCode -40522002 for 603168.SH
2017-02-24 12:27:46,019 - 20266 - INFO - crawl finished: 5678 for code 603168.SH
2017-02-24 12:27:46,022 - 20266 - INFO - close websocket
run offline spider "Daily Analysis Spider" finished

非法数据服务
  not sure about the exact reason, restarting the agent fixes this issue.

------------ 2017-02-23 19:00:19
crawl history data for daily market:
echo "2017 2 3 2017 2 21" > /tmp/spider.ranges
  spent 419w quota

> db.wind_wsd.find({rel_ipo_pct_chg : {$exists : true}}, {date : 1}).sort({date : -1})
{ "_id" : ObjectId("58ae8c812fb11c7495d307f5"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c292fb11c7495d307da"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c782fb11c7495d307df"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c7a2fb11c7495d307e3"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c7c2fb11c7495d307e7"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c7d2fb11c7495d307eb"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c7f2fb11c7495d307ef"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c802fb11c7495d307f3"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c822fb11c7495d307f7"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c842fb11c7495d307fb"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c852fb11c7495d307fe"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c862fb11c7495d30801"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c872fb11c7495d30804"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c882fb11c7495d30807"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c8a2fb11c7495d3080a"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c8b2fb11c7495d3080d"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c8c2fb11c7495d30810"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c8d2fb11c7495d30813"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c8f2fb11c7495d30816"), "date" : NumberLong("1487692800000") }
{ "_id" : ObjectId("58ae8c902fb11c7495d30819"), "date" : NumberLong("1487692800000") }
Type "it" for more
> db.wind_wsd.find({rel_ipo_pct_chg : {$exists : true}, date : {$lt : 1487692800000}}, {date : 1}).sort({date : -1})
{ "_id" : ObjectId("58a9c10a2fb11c7495d307d5"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a9c1092fb11c7495d307d4"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a9c1072fb11c7495d307ce"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a9c1082fb11c7495d307d0"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a9c1072fb11c7495d307cf"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a9c1082fb11c7495d307d1"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a9c1082fb11c7495d307d2"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a9c1092fb11c7495d307d3"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a9c10a2fb11c7495d307d6"), "date" : NumberLong("1487433600000") }
{ "_id" : ObjectId("58a86f8b2fb11c7495d307cb"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a86f8b2fb11c7495d307ca"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a86f892fb11c7495d307c4"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a86f892fb11c7495d307c6"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a86f892fb11c7495d307c5"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a86f8a2fb11c7495d307c7"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a86f8a2fb11c7495d307c8"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a86f8b2fb11c7495d307c9"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a86f8c2fb11c7495d307cc"), "date" : NumberLong("1487347200000") }
{ "_id" : ObjectId("58a71e1f2fb11c7495d2f068"), "date" : NumberLong("1487260800000") }
{ "_id" : ObjectId("58a71e102fb11c7495d2f04b"), "date" : NumberLong("1487260800000") }
Type "it" for more

---------- 2017-02-23 15:11:14
tried 5 accounts, we have got only 2000 quota for edb every 7 days. (every 7 rolling days)
  http://dajiangzhang.com/document#pointa_ea9f1e59-1915-4e0b-b4aa-2fcaecc7974f

----------- 2017-02-23 13:50:33
the data is sometimes corrupted, need to re-crawl:
2 / 41135
data is none: 
{'errorCode': -40520012, 'codeList': None, 'data': None, 'fieldList': None, 'timeList': None}
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/gevent/greenlet.py", line 534, in run
    result = self._run(*self.args, **self.kwargs)
  File "/home/kidd/workspace/abc/apes-crawler/websocket/client.py", line 64, in response
    callback(resp, **kwargs)
  File "/home/kidd/workspace/abc/apes-crawler/spiders/edbspider.py", line 412, in sample_response
    self.result_data_len = len(data['data'])
TypeError: object of type 'NoneType' has no len()
<Greenlet at 0x7f698860a550: response> failed with TypeError


---------- 2017-02-23 10:02:28
2017-02-22 19:51:37,832 - 29194 - ERROR - data request failed: {"cmd":"edb","param":["M5400671","2015-11-30","2015-12-2","Fill=Previous"],"id":7080,"quota_usage":5505,"type":"cmd-response","data":"{\"codeList\":[\"ErrorReport\"],\"fieldList\":[\"OUTMESSAGE\"],\"timeList\":[\"2017-02-22T19:51:39\"],\"data\":[\"CWSDService: quota exceeded.\"],\"errorCode\":-40522017}"}


--------- 2017-02-22 16:52:27
edb daily crawler fix for _get_measures: (culprit: even indexes having same frequency might have different updating time point every period)
  crawl all and only those indexes which do not have last time-period's data. ignore those already up-to-date.

# week: M5558006, day: M0020237, month: M0017126, year: M0041963, quater: M0012976, half-year: M5543440

sample data:
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 17:24:03,379 - 2850 - INFO - connect websocket
2017-02-22 17:24:03,448 - 2850 - INFO - connect finished
2017-02-22 17:24:03,449 - 2850 - INFO - 1: request cmd: {"cmd": "edb", "type": "cmd", "param": ["M5558006", "2017-02-21", "2017-02-21", "Fill=Previous"]}
2017-02-22 17:24:03,449 - 2850 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['M5558006'], 'data': [21.85], 'fieldList': ['CLOSE'], 'timeList': ['2017-02-03T00:00:00']}
2017-02-22 17:24:06,528 - 2850 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 17:24:37,564 - 5168 - INFO - connect websocket
2017-02-22 17:24:37,871 - 5168 - INFO - connect finished
2017-02-22 17:24:37,872 - 5168 - INFO - 1: request cmd: {"cmd": "edb", "type": "cmd", "param": ["M0020237", "2017-02-21", "2017-02-21", "Fill=Previous"]}
2017-02-22 17:24:37,872 - 5168 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['M0020237'], 'data': [159.8795], 'fieldList': ['CLOSE'], 'timeList': ['2017-02-21T00:00:00']}
2017-02-22 17:24:41,133 - 5168 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 17:24:46,904 - 5768 - INFO - connect websocket
2017-02-22 17:24:46,967 - 5768 - INFO - connect finished
2017-02-22 17:24:46,968 - 5768 - INFO - 1: request cmd: {"cmd": "edb", "type": "cmd", "param": ["M0017126", "2017-02-21", "2017-02-21", "Fill=Previous"]}
2017-02-22 17:24:46,968 - 5768 - INFO - request measures count: 1
{'errorCode': -40520012, 'codeList': None, 'data': None, 'fieldList': None, 'timeList': None}
2017-02-22 17:24:53,346 - 5768 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 17:25:03,336 - 6807 - INFO - connect websocket
2017-02-22 17:25:03,412 - 6807 - INFO - connect finished
2017-02-22 17:25:03,413 - 6807 - INFO - 1: request cmd: {"cmd": "edb", "type": "cmd", "param": ["M0017126", "2017-02-21", "2017-02-21", "Fill=Previous"]}
2017-02-22 17:25:03,414 - 6807 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['M0017126'], 'data': [51.3], 'fieldList': ['CLOSE'], 'timeList': ['2017-01-31T00:00:00']}
2017-02-22 17:25:07,498 - 6807 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 17:25:25,378 - 8211 - INFO - connect websocket
2017-02-22 17:25:25,450 - 8211 - INFO - connect finished
2017-02-22 17:25:25,451 - 8211 - INFO - 1: request cmd: {"cmd": "edb", "type": "cmd", "param": ["M0041963", "2017-02-21", "2017-02-21", "Fill=Previous"]}
2017-02-22 17:25:25,451 - 8211 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['M0041963'], 'data': [247860.1], 'fieldList': ['CLOSE'], 'timeList': ['2016-12-31T00:00:00']}
2017-02-22 17:25:26,737 - 8211 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 17:25:33,793 - 8742 - INFO - connect websocket
2017-02-22 17:25:33,856 - 8742 - INFO - connect finished
2017-02-22 17:25:33,857 - 8742 - INFO - 1: request cmd: {"cmd": "edb", "type": "cmd", "param": ["M0012976", "2017-02-21", "2017-02-21", "Fill=Previous"]}
2017-02-22 17:25:33,857 - 8742 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['M0012976'], 'data': [101.4], 'fieldList': ['CLOSE'], 'timeList': ['2016-12-31T00:00:00']}
2017-02-22 17:25:35,002 - 8742 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 17:25:42,089 - 9256 - INFO - connect websocket
2017-02-22 17:25:42,163 - 9256 - INFO - connect finished
2017-02-22 17:25:42,164 - 9256 - INFO - 1: request cmd: {"cmd": "edb", "type": "cmd", "param": ["M5543440", "2017-02-21", "2017-02-21", "Fill=Previous"]}
2017-02-22 17:25:42,164 - 9256 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['M5543440'], 'data': [3120.319687], 'fieldList': ['CLOSE'], 'timeList': ['2016-06-30T00:00:00']}
2017-02-22 17:25:45,281 - 9256 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 17:26:00,271 - 10326 - INFO - connect websocket
2017-02-22 17:26:00,329 - 10326 - INFO - connect finished
2017-02-22 17:26:00,331 - 10326 - INFO - 1: request cmd: {"cmd": "edb", "type": "cmd", "param": ["G0003758", "2017-02-21", "2017-02-21", "Fill=Previous"]}
2017-02-22 17:26:00,331 - 10326 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['G0003758'], 'data': [189.3], 'fieldList': ['CLOSE'], 'timeList': ['2016-12-31T00:00:00']}
2017-02-22 17:26:04,542 - 10326 - INFO - close websocket
run offline spider "EDB Data Spider" finished

--------- 2017-02-22 14:28:20
$ sudo python runcrawler.py offline "EDB Data Spider"
[sudo] password for kidd: 
['anhye@abcft.com']
2017-02-22 14:27:01,435 - 23656 - INFO - connect websocket
2017-02-22 14:27:01,520 - 23656 - INFO - connect finished
2017-02-22 14:27:01,520 - 23656 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["G0003749", "2016-08-17", "2017-02-17", "Fill=Previous"]}
2017-02-22 14:27:01,520 - 23656 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['G0003749'], 'data': [6196.4, 6154.9, 6041.9, 5944.7, 6003.9], 'fieldList': ['CLOSE'], 'timeList': ['2016-08-31T00:00:00', '2016-09-30T00:00:00', '2016-10-31T00:00:00', '2016-11-30T00:00:00', '2016-12-31T00:00:00']}
2017-02-22 14:27:02,613 - 23656 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 14:27:57,606 - 27060 - INFO - connect websocket
2017-02-22 14:27:57,683 - 27060 - INFO - connect finished
2017-02-22 14:27:57,683 - 27060 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["S2700349", "2016-08-17", "2017-02-17", "Fill=Previous"]}
2017-02-22 14:27:57,683 - 27060 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['S2700349'], 'data': [12541.0, 12734.0, 12965.0, 13083.0], 'fieldList': ['CLOSE'], 'timeList': ['2016-08-31T00:00:00', '2016-09-30T00:00:00', '2016-10-31T00:00:00', '2016-11-30T00:00:00']}
2017-02-22 14:27:57,861 - 27060 - INFO - close websocket
run offline spider "EDB Data Spider" finished

$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 14:30:11,737 - 2976 - INFO - connect websocket
2017-02-22 14:30:11,795 - 2976 - INFO - connect finished
2017-02-22 14:30:11,796 - 2976 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["S2700349,G0003749", "2016-10-17", "2017-02-17", "Fill=Previous"]}
2017-02-22 14:30:11,796 - 2976 - INFO - request measures count: 2
{'errorCode': 0, 'codeList': ['S2700349', 'G0003749'], 'data': [12965.0, 6041.9, 13083.0, 5944.7, 13083.0, 6003.9], 'fieldList': ['CLOSE'], 'timeList': ['2016-10-31T00:00:00', '2016-11-30T00:00:00', '2016-12-31T00:00:00']}
2017-02-22 14:30:12,648 - 2976 - INFO - close websocket
run offline spider "EDB Data Spider" finished

without "'extraParams': 'Fill=Previous',"
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 14:29:38,330 - 785 - INFO - connect websocket
2017-02-22 14:29:38,403 - 785 - INFO - connect finished
2017-02-22 14:29:38,403 - 785 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["S2700349,G0003749", "2016-10-17", "2017-02-17"]}
2017-02-22 14:29:38,403 - 785 - INFO - request measures count: 2
{'errorCode': -40520012, 'codeList': None, 'data': None, 'fieldList': None, 'timeList': None}
2017-02-22 14:29:43,420 - 785 - INFO - close websocket
run offline spider "EDB Data Spider" finished

with "'extraParams': 'Fill=Previous',", need to fix the db update strategy:
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-22 14:38:51,977 - 2716 - INFO - connect websocket
2017-02-22 14:38:53,046 - 2716 - INFO - connect finished
2017-02-22 14:38:53,046 - 2716 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["S2700349,G0003749", "2017-2-17", "2017-2-17", "Fill=Previous"]}
2017-02-22 14:38:53,046 - 2716 - INFO - request measures count: 2
{'errorCode': 0, 'codeList': ['S2700349', 'G0003749'], 'data': [13083.0, 'NaN', 13083.0, 6003.9], 'fieldList': ['CLOSE'], 'timeList': ['2016-11-30T00:00:00', '2016-12-31T00:00:00']}
2017-02-22 14:38:54,529 - 2716 - INFO - close websocket
run offline spider "EDB Data Spider" finished

$ sudo python runcrawler.py offline "EDB Data Spider"
[sudo] password for kidd: 
['anhye@abcft.com']
2017-02-22 16:04:46,030 - 31473 - INFO - connect websocket
2017-02-22 16:04:46,348 - 31473 - INFO - connect finished
2017-02-22 16:04:46,348 - 31473 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["S2700349", "2017-2-17", "2017-2-17", "Fill=Previous"]}
2017-02-22 16:04:46,348 - 31473 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['S2700349'], 'data': [13083.0], 'fieldList': ['CLOSE'], 'timeList': ['2016-11-30T00:00:00']}
2017-02-22 16:04:46,439 - 31473 - INFO - response process succeed: EDB Data Spider
2017-02-22 16:04:46,439 - 31473 - INFO - close websocket
run offline spider "EDB Data Spider" finished

------------- 2017-02-22 10:43:22
120.55.112.51(10.168.152.131),
120.26.169.235(10.171.177.232)，
已经有两台机器才抓了一点点就超限了

node:
[2017-02-22 10:42:41.767] [INFO] app - cmd: {"cmd":"edb","param":["S0190177","2016-11-30","2016-12-2","Fill=Previous"],"id":7933,"quota_usage":8186,"type":"cmd-response","data":"{\"codeList\":[\"ErrorReport\"],\"fieldList\":[\"OUTMESSAGE\"],\"timeList\":[\"2017-02-22T10:42:38\"],\"data\":[\"CWSDService: quota exceeded.\"],\"errorCode\":-40522017}"}
[2017-02-22 10:42:41.767] [INFO] app - quota usage: 8186
[2017-02-22 10:42:41.768] [INFO] app - reply data to manager: {"cmd":"edb","param":["S0190177","2016-11-30","2016-12-2","Fill=Previous"],"id":7933,"quota_usage":8186,"type":"cmd-response","data":"{\"codeList\":[\"ErrorReport\"],\"fieldList\":[\"OUTMESSAGE\"],\"timeList\":[\"2017-02-22T10:42:38\"],\"data\":[\"CWSDService: quota exceeded.\"],\"errorCode\":-40522017}"} lEQ0qG8ytZrwkDzKF+BHBA==
[2017-02-22 10:42:41.768] [INFO] app - update error status to: -40522017 for PkGO/wJY4q/Q7JG5pjkCOg==
[2017-02-22 10:42:42.277] [INFO] app - connection closed  lEQ0qG8ytZrwkDzKF+BHBA==

crawler:
$ sudo python runcrawler.py offline "EDB Data Spider"
[sudo] password for kidd: 
['anhye@abcft.com']
2017-02-22 10:42:09,311 - 28249 - INFO - connect websocket
2017-02-22 10:42:09,373 - 28249 - INFO - connect finished
crawling day data...
1 / 1
2017-02-22 10:42:40,338 - 28249 - ERROR - data request failed: {"cmd":"edb","param":["S0190177","2016-11-30","2016-12-2","Fill=Previous"],"id":7933,"quota_usage":8186,"type":"cmd-response","data":"{\"codeList\":[\"ErrorReport\"],\"fieldList\":[\"OUTMESSAGE\"],\"timeList\":[\"2017-02-22T10:42:38\"],\"data\":[\"CWSDService: quota exceeded.\"],\"errorCode\":-40522017}"}


-------------- 2017-02-21 20:43:12
  日 周 月 季 半年  年 total 帐户周
27年消耗 69349635  10793952  12972636  318708  2376  573804  94011111  15.6685185


total: 220, day count: 0. M5513086 not
221 / 46307

------- 2017-02-21 20:19:48
result for day-field with none-day field:
2016-12-20 - 2016-12-22
{'errorCode': 0, 'codeList': ['M5567891', 'M0020237'], 'data': ['NaN', 159.4901, 'NaN', 159.5329, 'NaN', 159.7158, 84582.8, 159.7158], 'fieldList': ['CLOSE'], 'timeList': ['2016-12-20T00:00:00', '2016-12-21T00:00:00', '2016-12-22T00:00:00', '2016-12-31T00:00:00']}
finished crawling day data

result for a day field:
{'errorCode': 0, 'codeList': ['M0020237'], 'data': [159.4901, 159.5329, 159.7158], 'fieldList': ['CLOSE'], 'timeList': ['2016-12-20T00:00:00', '2016-12-21T00:00:00', '2016-12-22T00:00:00']}
finished crawling day data

result for a none-day field:
2016-12-20 - 2016-12-22
{'errorCode': 0, 'codeList': ['M5567891'], 'data': [84582.8], 'fieldList': ['CLOSE'], 'timeList': ['2016-12-31T00:00:00']}
finished crawling day data


------- 2017-02-20 16:28:02
房地产销售及投资
刷新
添加图表
  updating too slow, e.g., 

  $ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-20 16:23:11,898 - 9070 - INFO - connect websocket
2017-02-20 16:23:11,973 - 9070 - INFO - connect finished
2017-02-20 16:23:11,974 - 9070 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["S2700452", "2016-06-17", "2017-02-17", "Fill=Previous"]}
2017-02-20 16:23:11,974 - 9070 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['S2700452'], 'data': [22.0, 30.0, 29.0, 18.0, 21.0, 19.0], 'fieldList': ['CLOSE'], 'timeList': ['2016-06-30T00:00:00', '2016-07-31T00:00:00', '2016-08-31T00:00:00', '2016-09-30T00:00:00', '2016-10-31T00:00:00', '2016-11-30T00:00:00']}
2017-02-20 16:23:13,060 - 9070 - INFO - close websocket
run offline spider "EDB Data Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "EDB Data Spider"
['anhye@abcft.com']
2017-02-20 16:26:57,177 - 22893 - INFO - connect websocket
2017-02-20 16:26:57,239 - 22893 - INFO - connect finished
2017-02-20 16:26:57,240 - 22893 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["S2700453", "2016-06-17", "2017-02-17", "Fill=Previous"]}
2017-02-20 16:26:57,240 - 22893 - INFO - request measures count: 1
{'errorCode': 0, 'codeList': ['S2700453'], 'data': [1.32, 1.63, 2.17, 2.83, 1.65, 0.88], 'fieldList': ['CLOSE'], 'timeList': ['2016-06-30T00:00:00', '2016-07-31T00:00:00', '2016-08-31T00:00:00', '2016-09-30T00:00:00', '2016-10-31T00:00:00', '2016-11-30T00:00:00']}
2017-02-20 16:26:57,893 - 22893 - INFO - close websocket
run offline spider "EDB Data Spider" finished


---------- 2017-02-20 15:34:49
db.wind_edb.find({M0012990 : {$exists : true}}, {date : 1, dateObj : 1}).sort({date : -1}).limit(3)

> db.charts.find({_id : 4331}, {thumbnail : 0}).limit(1).pretty()
db.work_sheet.find({_id : ObjectId("58a68a775d32cb37ee262bc7")}, {raw : 0, thumbnail : 0, data : 0})

--------------- 2017-02-17 16:23:24
[ahye@tz8-demo ~]$ mongoexport -h 10.51.3.146 --port 3017 -d apes -c wind_edb -o ~/wind_edb_online_bak.dat
2017-02-17T16:23:04.309+0800  connected to: 10.51.3.146:3017
2017-02-17T16:23:04.740+0800  exported 5785 records

[ahye@tz8-demo ~]$ mongoimport -h 10.51.3.146 --port 3017 -d apes -c wind_edb wind_edb.dat
2017-02-17T16:26:13.824+0800  connected to: 10.51.3.146:3017
2017-02-17T16:26:14.342+0800  imported 5836 documents

----------- 2017-02-17 11:56:29
db.wind_edb.find().count()
5785

local:
> db.wind_edb.find().count()
5824
> 

----------- 2017-02-17 11:25:02
online measures:
> use apes
switched to db apes
> db.measures.find({datasetId : 'wind_edb', frequency : 'year'}).count()
6
> db.measures.find({datasetId : 'wind_edb', frequency : 'quarter'}).count()
11
> db.measures.find({datasetId : 'wind_edb'}).count()
675

$ sudo python runcrawler.py offline "Daily Market Spider"
['anhye@abcft.com']
2017-02-17 12:37:12,796 - 19045 - INFO - connect websocket
2017-02-17 12:37:14,088 - 19045 - INFO - connect finished
total count: 242371
None
2017-02-17 12:37:14,943 - 19045 - INFO - close websocket
run offline spider "Daily Market Spider" finished
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ sudo python runcrawler.py offline "Daily Market Spider"
[sudo] password for kidd: 
['anhye@abcft.com']
2017-02-17 13:53:33,073 - 13675 - INFO - connect websocket
2017-02-17 13:53:33,149 - 13675 - INFO - connect finished
total count: 248905
None
2017-02-17 13:53:34,014 - 13675 - INFO - close websocket
run offline spider "Daily Market Spider" finished

---------- 2017-02-15 20:09:28
$ sudo python runcrawler.py offline "EDB Data Spider"

sudo python runcrawler.py offline "Daily Market Spider"

---------- 2017-02-09 15:18:06
> db.wind_wsd.find({windcode : '000001.SH'}).sort({date : -1}).limit(1).pretty()

{
  "_id" : ObjectId("58844dd8878544f6a1e51fe2"),
  "date" : NumberLong("1484841600000"),
  "windcode" : "000001.SH",
  "close" : 3123.1389,
  "sec_name" : "上证综指"
}

2017/1/20 0:0:0



-------- 2017-02-04 12:36:04
switch ip/port to 10.51.3.146/3017 in the code before execution
[root@HyperLedger001 ahye]# python /niub/bj/apes-crawler/tools/import_edb_measurements.py

after exec: 
  > db.measures.find({datasetId : 'wind_edb'}).count()
  643

db.wind_edb.find().count()
5778

backup hyperledger server:
  db.wind_edb.find().count()
  5777

> db.last_crawl.find({code : 'M0061614'})
{ "_id" : ObjectId("58956db3f8296a252d5788a9"), "code" : "M0061614", "key" : "edb_data", "date" : "2017-01-29", "finished" : true, "crawl_time" : ISODate("2017-02-04T13:59:15.368Z") }

$ sudo python runcrawler.py restart
['anhye@abcft.com']
Stopping...
Stopped
Starting...
Started
/usr/local/lib/python2.7/dist-packages/pymongo/topology.py:145: UserWarning: MongoClient opened before fork. Create MongoClient with connect=False, or create client after forking. See PyMongo's documentation for details: http://api.mongodb.org/python/current/faq.html#pymongo-fork-safe>
  "MongoClient opened before fork. Create MongoClient "
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ netstat -apn | grep 8765
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 0.0.0.0:8765            0.0.0.0:*               LISTEN      - 

---------- 2017-01-24 16:39:10
the return fields in the data is in capital format!!!

---------- 2017-01-24 14:31:07
[root@bj-tz8-db001 ~]# ps -ef | grep python
root       344     1  0 13:03 ?        00:00:01 python runcrawler.py restart
root       346   344  0 13:03 ?        00:00:00 python runcrawler.py restart
root       347   344  0 13:03 ?        00:00:00 python runcrawler.py restart
root       349   344  0 13:03 ?        00:00:00 python runcrawler.py restart
root       351   344  0 13:03 ?        00:00:00 python runcrawler.py restart
root       352   344  0 13:03 ?        00:00:00 python runcrawler.py restart
root       354   344  0 13:03 ?        00:00:00 python runcrawler.py restart
root       358   344  0 13:03 ?        00:00:00 python runcrawler.py restart
root       362   344  0 13:03 ?        00:00:00 python runcrawler.py restart
root       394 31316  0 13:08 pts/0    00:00:00 grep python


------------ 2017-01-23 15:18:25
http://120.26.95.121/views/test/charts.html#14

11 单条曲线图 "指数" close X
12  风格指数一周表现 close [中证100, 中证500] X
13  二元曲线图 close
  ":label1" : { "default" : "中证100" }, ":label2" : { "default" : "中证500" }
14  2行表格
15  三季报业绩预警情况
16  不同板块的业绩预警向好比例
25  上证综指一周表现
26  二元曲线
27  New template
28  行业业绩预警向好比例
29  已经披露3季报业绩公司可比业绩增速
30  已经披露3季报业绩公司业绩增速分行业情况
31  风格指数表现1W
32  风格指数表现YTD
33  A股行业表现1W
34  A股行业表现YTD
35  市场换手率
36  票据直贴利率
37  公开市场操作：货币净投放
38  10年到期国债收益率
39  M1/M2同比
40  实际利率变化
41  融资余额
42  融资余额增加
47  风格指数TTM市盈率
59  相对股价
60  财务报表 - 利润表
61  业绩回顾
62  财务报表 - 资产负债表
63  财务报表 - 现金流量表
72  业绩回顾2
73  主要财务比率 - 成长能力
79  主要财务比率 - 盈利能力
100 主要财务比率 - 偿债能力
101 主要财务比率 - 回报率分析
102 主要财务比率 - 每股指标
103 主要财务比率 - 估值分析
120 单季度利润表分析
137 单季度利润表分析 - 财务指标
144 前三季度业绩概览
189 非银行板块TTM市盈率
190 沪股通累计净买入(十亿元)
197 上周新投资者数量
419 风格指数表现1W



------------ 2017-01-22 18:11:20
US treasury holdings - 美国:外国投资者持有美国国债
  cannot found:
    持有美债单月变化


--------- 2017-01-22 14:33:31
> db.last_fail_point.find()
{ "_id" : ObjectId("58845032b14e9901adf62c27"), "spider" : "offline_daily_analysis", "start" : "2017-01-20", "code" : "002069.SZ", "end" : "2017-01-20" }


----------- 2017-01-20 10:28:48
rdf:
  C:\Users\Administrator\Downloads\wind\fileSync.windows\filesync.windows


---------- 2017-01-19 18:23:05
120.55.112.51 can visit 120.26.95.121 via 10.x.x.x (intranet) while 120.26.168.180 could not.
  but 120.26.168.180 is allowed via iptables with public ip 120.26.95.121 directly.

[root@tz8-demo wsservice]# sudo iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere            state RELATED,ESTABLISHED 
ACCEPT     icmp --  anywhere             anywhere            
ACCEPT     tcp  --  113.57.168.243       anywhere            tcp dpt:ssh 
ACCEPT     tcp  --  123.126.24.14        anywhere            tcp dpt:ssh 
ACCEPT     tcp  --  120.26.245.253       anywhere            tcp dpt:xmltec-xmlmail 
ACCEPT     tcp  --  123.126.24.14        anywhere            tcp dpt:xmltec-xmlmail 
ACCEPT     tcp  --  120.26.168.180       anywhere            tcp dpt:xmltec-xmlmail 
ACCEPT     tcp  --  anywhere             anywhere            tcp dpt:http 
ACCEPT     tcp  --  anywhere             anywhere            tcp dpt:https 
ACCEPT     tcp  --  anywhere             anywhere            tcp dpt:glrpc 
ACCEPT     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
REJECT     all  --  anywhere             anywhere            reject-with icmp-host-prohibited 

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination 

----------- 2017-01-18 14:14:20
sudo python runcrawler.py offline "Daily Market Spider"

csharp:
if (wd != null)
  {
      if (wd.errorCode == 0)
      {
          Console.WriteLine("data size: " + wd.data);
      }
                        
Message: {"cmd":"wsd","type":"cmd","param":["000903.SH","close,sec_name","2017-01-13","2017-01-14"],"id":1}
data size: System.Object[]


------------ 2017-01-14 16:42:29
there was a big bug in the code, which will lead to all times having same data:
fix:
        field_size = len(fields)
        for i in range(len(time_list)):
            date = int(time.mktime(time.strptime(time_list[i],"%Y-%m-%dT%H:%M:%S")) * 1000)
            doc = {
              'date': date
            }
            for j in range(field_size):
                data_index = field_size * i + j
                if isinstance(d1[data_index], datetime):
                    doc[fields[j].lower()] = int(time.mktime(d1[data_index].timetuple()) * 1000)
                else:
                    doc[fields[j].lower()] = getNoneOnNan(d1[data_index])

need to update today's close data for both: 风格指数表现 & 上证综指一周表现
  also need to re-crawl this week's history data crawl

----------- 2017-01-14 12:09:50
$ mongoexport -h 10.12.6.6 -d apes -c wind_wsd -f "close,date,windcode" -q '{date : {$gte : 1480262400000, $lte : 1484150400000}, "windcode":{"$in":["000001.SH","000903.SH","000904.SH","000905.SH","000906.SH","000300.SH","399001.SZ","399005.SZ","399006.SZ"]}}' -o ~/tmp/wind_wsd_02.dat
connected to: 10.12.6.6
exported 414 records

db.wind_wsd.find({date : {$gte : 1480262400000, $lte : 1484150400000}}).count()
414

def upsert_wind_wsd_close(doc):
  info_connection=pymongo.MongoClient('10.12.6.43', 27017)
  info_db=info_connection.apes
  info_col=info_db.wind_wsd
  time = datetime.datetime.fromtimestamp(doc['date'] / 1000)
  print 'updating ' + doc['windcode'] + 'data on ' + str(time)
  # print doc['date']
  del doc['_id']
  info_col.update({'windcode': doc['windcode'], 'date' : doc['date']}, {'$set': doc}, upsert=True)

if __name__ == "__main__":
  file = open("/home/kidd/tmp/wind_wsd_02.dat")
  try:
      # content = file.read()
      docs = file.readlines()
      print 'got ' + str(len(docs)) + ' lines'
      for doc in docs:
        upsert_wind_wsd_close(simplejson.loads(doc.replace('\n', '')))
  finally:
      file.close()

$ mongoexport -h 10.12.6.6 -d apes -c wind_wsd -f "close,date,windcode" -q "{date : 1451577600000}" -o ~/tmp/wind_wsd.dat
connected to: 10.12.6.6
exported 9 records

> db.wind_wsd.find({date : 1451577600000}).count()
9


-------- 2017-01-14 10:09:24

the 2nd account has run out of quota:
[ahye@bj-tz8-db001 ~]$ tail /tmp/spider_Daily_Analysis_Spider.log
2017-01-14 00:11:49,448 - 10213 - INFO - reset connection
2017-01-14 00:11:49,449 - 10213 - INFO - close websocket
2017-01-14 00:11:49,449 - 10213 - INFO - connect websocket
2017-01-14 00:11:49,452 - 10213 - INFO - connect finished
2017-01-14 00:11:49,452 - 10213 - INFO - reset connection finished
2017-01-14 00:11:49,452 - 10213 - INFO - crawl for DailyAnalysisSpider
2017-01-14 00:11:49,508 - 10213 - INFO - send websocket request: 6374
2017-01-14 00:11:49,709 - 10213 - INFO - errorCode -40522017 for 000960.SZ
2017-01-14 00:11:49,709 - 10213 - INFO - crawl finished: 6374 for code 000960.SZ
2017-01-14 00:11:51,711 - 10213 - INFO - retries exceeding limit, exitting ......

try the local test account to crawl demo data:



------------- 2017-01-12 16:52:48
[ahye@bj-tz8-db001 ~]$ tail /tmp/spider_Daily_Market_Spider.log
2017-01-12 12:02:09,499 - 9690 - INFO - reset connection
2017-01-12 12:02:09,499 - 9690 - INFO - close websocket
2017-01-12 12:02:09,500 - 9690 - INFO - connect websocket
2017-01-12 12:02:09,504 - 9690 - INFO - connect finished
2017-01-12 12:02:09,504 - 9690 - INFO - reset connection finished
2017-01-12 12:02:09,504 - 9690 - INFO - crawl for DailyMarketSpider
2017-01-12 12:02:09,563 - 9690 - INFO - send websocket request: 2066
2017-01-12 12:02:09,647 - 9690 - INFO - errorCode -40522007 for 600219.SH
2017-01-12 12:02:09,648 - 9690 - INFO - crawl finished: 2066 for code 600219.SH
2017-01-12 12:02:11,649 - 9690 - INFO - retries exceeding limit, exitting ......
[ahye@bj-tz8-db001 ~]$ tail /tmp/spider_Daily_Analysis_Spider.log
2017-01-12 12:13:50,597 - 9691 - INFO - reset connection
2017-01-12 12:13:50,597 - 9691 - INFO - close websocket
2017-01-12 12:13:50,598 - 9691 - INFO - connect websocket
2017-01-12 12:13:50,600 - 9691 - INFO - connect finished
2017-01-12 12:13:50,600 - 9691 - INFO - reset connection finished
2017-01-12 12:13:50,601 - 9691 - INFO - crawl for DailyAnalysisSpider
2017-01-12 12:13:50,667 - 9691 - INFO - send websocket request: 2098
2017-01-12 12:13:51,054 - 9691 - INFO - errorCode -40522007 for 600241.SH
2017-01-12 12:13:51,054 - 9691 - INFO - crawl finished: 2098 for code 600241.SH
2017-01-12 12:13:51,055 - 9691 - INFO - retries exceeding limit, exitting ......

-40522007 不支持的指标

export CRAWLER_ENV=release
python db/cmds.py upsert_last_fail_point daily_market 2017-01-11 2017-01-11 600219.SH
python db/cmds.py upsert_last_fail_point daily_analysis 2017-01-11 2017-01-11 600241.SH


------------- 2017-01-12 10:20:03
[ahye@bj-tz8-db001 ~]$ tail -f /tmp/spider_Daily_Market_Spider.log
2017-01-11 11:41:24,817 - 3681 - INFO - need to upsert daily market for 603998.SH
2017-01-11 11:41:24,817 - 3681 - INFO - crawl finished: 2968 for code 603998.SH
2017-01-11 11:41:24,817 - 3681 - INFO - send websocket request: 2969
2017-01-11 11:41:25,741 - 3681 - INFO - data size: 105 for 603999.SH
2017-01-11 11:41:25,799 - 3681 - INFO - need to upsert daily market for 603999.SH
2017-01-11 11:41:25,799 - 3681 - INFO - crawl finished: 2969 for code 603999.SH
2017-01-11 11:41:25,800 - 3681 - INFO - finished crawling
2017-01-12 00:00:01,271 - 3681 - INFO - crawl for DailyMarketSpider
2017-01-12 00:00:06,280 - 3681 - INFO - send websocket request error: 10.51.3.146:3017: [Errno 104] Connection reset by peer
2017-01-12 00:00:06,281 - 3681 - INFO - code exception, to exit

2017-01-12 10:23:34
restarted the crawler.

quota left for this week:
600 - 230 - 75 = 295
3 days left to crawl for daily data, which is around 225w

-------------- 2017-01-11 12:12:39

crawler_log

2017-01-11 12:11:21
  started to crawl daily analysis for "2016 12 19 2016 12 29". failed yesterday due to quota limit. daily market crawled yesterday. switched account this morning
  [root@bj-tz8-db001 apes-crawler]# sh runoffline.sh

> use apes
switched to db apes
> db.wind_analysis.find().count()
37559
> db.wind_analysis.find().count()
37566
> db.wind_analysis.find().count()
37580
> db.wind_analysis.find().count()
37594
> db.wind_analysis.find().count()
39141
> db.wind_analysis.find().count()
53722

around 230w

[root@bj-tz8-db001 apes-crawler]# tail nohup.out 
2017-01-11 12:39:03,191 - 5764 - INFO - send websocket request: 2449
2017-01-11 12:39:03,380 - 5764 - INFO - data size: 1296 for 603998.SH
2017-01-11 12:39:03,702 - 5764 - INFO - need to upsert daily analysis for 603998.SH
2017-01-11 12:39:03,703 - 5764 - INFO - crawl finished: 2449 for code 603998.SH
2017-01-11 12:39:03,703 - 5764 - INFO - send websocket request: 2450
2017-01-11 12:39:04,254 - 5764 - INFO - data size: 1296 for 603999.SH
2017-01-11 12:39:04,588 - 5764 - INFO - need to upsert daily analysis for 603999.SH
2017-01-11 12:39:04,588 - 5764 - INFO - crawl finished: 2450 for code 603999.SH
2017-01-11 12:39:04,588 - 5764 - INFO - close websocket
run offline spider "Daily Analysis Spider" finished
[root@bj-tz8-db001 apes-crawler]# pwd
/niub/bj/apes-crawler/apes-crawler

-------------- 2017-01-10 20:20:03
> db.wind_wsd.find().count()
53523
> db.wind_wsd.find().count()
53523
> db.wind_wsd.find().count()
53577
> db.wind_wsd.find().count()
53577
> db.wind_wsd.find().count()
53577
> db.wind_wsd.find().count()
53577
> db.wind_wsd.find().count()
53608
> db.wind_wsd.find().count()
53650
> db.wind_wsd.find().count()
53771
> db.wind_wsd.find().count()
74286

105 indexes for daily market

144 indexes for daily analysis

> db.wind_analysis.find().count()
30299
wind_analysis

> db.last_fail_point.find().pretty()
{
  "_id" : ObjectId("5874d836b14e9901adf4b03b"),
  "spider" : "offline_daily_analysis",
  "start" : "2016-12-19",
  "code" : "002054.SZ",
  "end" : "2016-12-29"
}

-40522017 数据提取量超限
[root@bj-tz8-db001 apes-crawler]# tail nohup.out 
2017-01-10 20:48:52,965 - 2171 - INFO - crawl finished: 518 for code 002051.SZ
2017-01-10 20:48:52,965 - 2171 - INFO - send websocket request: 519
2017-01-10 20:48:53,483 - 2171 - INFO - data size: 1296 for 002052.SZ
2017-01-10 20:48:53,691 - 2171 - INFO - need to upsert daily analysis for 002052.SZ
2017-01-10 20:48:53,691 - 2171 - INFO - crawl finished: 519 for code 002052.SZ
2017-01-10 20:48:53,691 - 2171 - INFO - send websocket request: 520
2017-01-10 20:48:53,985 - 2171 - INFO - errorCode -40522017 for 002054.SZ
2017-01-10 20:48:53,985 - 2171 - INFO - crawl finished: 520 for code 002054.SZ
2017-01-10 20:48:54,014 - 2171 - INFO - close websocket

switched account on: 2017-01-11 10:40:38
  and re-deployed the crawler to restart the spiders, which will exit in case of errors like -40522017
  need to restart history data crawler for daily analysis



------------- 2017-01-11 15:57:18

def _generate_date_range(self, months)
months = 30 => (2017-01-11 15:57:55)
  2014-03-31, 2016-12-31

--------- 

1) 市场行情/证券分析 (每日数据)

全部a股每天：(107（市场行情指标数量）+ 146（证券分析指标数量）) * 2969（A股股票数量）= 751157
        每周：751157 * 5 = 3755785 = 376w / 周, 1504w / 4周
全部股票每天：(107（市场行情指标数量）+ 146（证券分析指标数量）) * 4478（全部股票数量）= 1132924
      每周：1132924 * 5 = 5664670 = 567w / 周, 2268w / 4周

2) 预测评级/财务数据/权益事件 (季度数据)
全部a股: 每次抓取完整数据需使用配额：(281（预测评级指标数量）+ 1435（财务数据指标数量）+ 235（权益事件指标数量）) * 2969（A股股票数量）= 5792519 = 580w / 季度, 6960w / 3年
全部股票: 每次抓取完整数据需使用配额：(281（预测评级指标数量）+ 1435（财务数据指标数量）+ 235（权益事件指标数量）) * 4478（A股股票数量）= 8736578 = 874w / 季度, 10488w / 3年


---------- 2017-02-03 15:09:53
[2017-02-03 15:08:33.028] [INFO] app - send to agent:  { cmd: 'wsd',
  type: 'cmd',
  param: [ '000001.SH', 'close,sec_name', '2017-01-13', '2017-01-13' ],
  id: 6410 }
[2017-02-03 15:08:33.030] [ERROR] app - Error  { [Error: read ECONNRESET] code: 'ECONNRESET', errno: 'ECONNRESET', syscall: 'read' }
Error: read ECONNRESET
    at exports._errnoException (util.js:907:11)
    at TCP.onread (net.js:557:26)
[2017-02-03 15:08:33.032] [INFO] app - connection closed  1FvjrufgZHOp1Xjxh7i8rQ==

[2017-02-03 16:39:25.318] [ERROR] app - Error  { [Error: read ECONNRESET] code: 'ECONNRESET', errno: 'ECONNRESET', syscall: 'read' }
Error: read ECONNRESET
    at exports._errnoException (util.js:907:11)
    at TCP.onread (net.js:557:26)
[2017-02-03 16:39:25.319] [INFO] app - connection closed  JMZiZipq5GHgHxdL+GuO1g==


---------- 2017-02-03 19:58:36
weird edb return data:
  the requesting date is 2017-02-02, while the returned dates are ['2015-09-24T00:00:00', '2015-10-24T00:00:00', '2016-01-29T00:00:00', '2016-08-20T00:00:00', '2016-10-06T00:00:00', '2016-12-30T00:00:00', '2017-01-24T00:00:00', '2017-02-02T00:00:00', '2017-02-03T00:00:00']

2017-02-03 19:57:33,025 - 535 - INFO - request cmd: {"cmd": "edb", "type": "cmd", "param": ["M0061577,M0043757,M0009808,M0060448,M0060450,M1004136,M1004677,M1004829,S0059741,S0059742,S0059743,S0059744,S0059745,S0059746,M0057946,S0059747,M0057947,S0059748,M1000165,M1004678,S0059749,S0059750,S0059751,S0059752,M1004711,M1000170,M0061578,M0061575,M0061576,M0061579,M0067160,G0005431,G0005432,G0001672,G0001667,S0000192,S0000193,M0017138,M0017139,M0017140,M0017141,M0017142,M0017143,M0017144,M0017145,M1004135,M0043761,M0043764,M0043797,M0043758,M0043773,M0043762,M0057944,M0043763,M0057945,M0043774,M1000149,M0043796,M0043759,M0043760,M1000154,G0008386,M0000185,M0000206,M0000199,M0000200,M0000201,M0000204,M0000271,S5116608,S5116609,S5116610,S5116611,S5116612,S5116613,S5116614,S5103920,S5113501,S5113516,S5113517,M0020188", "2017-02-02", "2017-02-02", "Fill=Previous"]}

$ {'errorCode': 0, 'codeList': ['M0061577', 'M0043757', 'M0009808', 'M0060448', 'M0060450', 'M1004136', 'M1004677', 'M1004829', 'S0059741', 'S0059742', 'S0059743', 'S0059744', 'S0059745', 'S0059746', 'M0057946', 'S0059747', 'M0057947', 'S0059748', 'M1000165', 'M1004678', 'S0059749', 'S0059750', 'S0059751', 'S0059752', 'M1004711', 'M1000170', 'M0061578', 'M0061575', 'M0061576', 'M0061579', 'M0067160', 'G0005431', 'G0005432', 'G0001672', 'G0001667', 'S0000192', 'S0000193', 'M0017138', 'M0017139', 'M0017140', 'M0017141', 'M0017142', 'M0017143', 'M0017144', 'M0017145', 'M1004135', 'M0043761', 'M0043764', 'M0043797', 'M0043758', 'M0043773', 'M0043762', 'M0057944', 'M0043763', 'M0057945', 'M0043774', 'M1000149', 'M0043796', 'M0043759', 'M0043760', 'M1000154', 'G0008386', 'M0000185', 'M0000206', 'M0000199', 'M0000200', 'M0000201', 'M0000204', 'M0000271', 'S5116608', 'S5116609', 'S5116610', 'S5116611', 'S5116612', 'S5116613', 'S5116614', 'S5103920', 'S5113501', 'S5113516', 'S5113517', 'M0020188'], 'data': [......], 'fieldList': ['CLOSE'], 'timeList': ['2015-09-24T00:00:00', '2015-10-24T00:00:00', '2016-01-29T00:00:00', '2016-08-20T00:00:00', '2016-10-06T00:00:00', '2016-12-30T00:00:00', '2017-01-24T00:00:00', '2017-02-02T00:00:00', '2017-02-03T00:00:00']}

------------ 2017-02-04 10:44:46
> db.wind_edb.find().sort({date : -1}).limit(10).pretty()
{
  "_id" : ObjectId("5893fe06ca6d38f77b4f1ff9"),
  "date" : "2017-01-24T00:00:00",
  "M0009808" : 1.5,
  "M0060450" : 3.4,
  "M0043757" : 2.8419,
  "M0060448" : 600,
  "M0061577" : 4.05
}
{
  "_id" : ObjectId("5893fe07ca6d38f77b4f1ffa"),
  "date" : "2016-12-31T00:00:00",
  "M0000612" : 2.1,
  "M0009973" : 10400,
  "M0001385" : 11.3,
  "M0010049" : 30105.17,
  "M0001383" : 21.4
}
{
  "_id" : ObjectId("5893fe06ca6d38f77b4f1ff8"),
  "date" : "2016-01-29T00:00:00",
  "M0009808" : 1.5,
  "M0060450" : 3.4,
  "M0043757" : 2.8419,
  "M0060448" : 600,
  "M0061577" : "NaN"
}
{
  "_id" : ObjectId("5893fe06ca6d38f77b4f1ff7"),
  "date" : "2015-10-24T00:00:00",
  "M0009808" : 1.5,
  "M0060450" : 3.4,
  "M0043757" : "NaN",
  "M0060448" : 600,
  "M0061577" : "NaN"
}
{
  "_id" : ObjectId("5893fe06ca6d38f77b4f1ff6"),
  "date" : "2015-09-24T00:00:00",
  "M0009808" : "NaN",
  "M0060450" : 3.4,
  "M0043757" : "NaN",
  "M0060448" : 600,
  "M0061577" : "NaN"
}
{
  "_id" : ObjectId("58870383878544f6a1e5311e"),
  "date" : NumberLong("1485100800000"),
  "M0061577" : 3.95
}
{
  "_id" : ObjectId("58731fd2878544f6a1e470df"),
  "date" : NumberLong("1483632000000"),
  "M0009808" : 1.5,
  "M0060450" : 3.4,
  "M0043757" : 2.8419,
  "M0060448" : 600,
  "M0061577" : 3.5,
  "dateObj" : ISODate("2017-01-06T00:00:00Z")
}
{
  "_id" : ObjectId("58731fd2878544f6a1e470de"),
  "date" : NumberLong("1483459200000"),
  "M0009808" : 1.5,
  "M0060450" : 3.4,
  "M0043757" : 2.8419,
  "M0060448" : 600,
  "M0061577" : 3.75,
  "dateObj" : ISODate("2017-01-04T00:00:00Z")
}
{
  "_id" : ObjectId("58731fd2878544f6a1e470dd"),
  "date" : NumberLong("1483372800000"),
  "M0009808" : 1.5,
  "M0060450" : 3.4,
  "M0043757" : 2.8419,
  "M0060448" : 600,
  "M0061577" : 2.85,
  "dateObj" : ISODate("2017-01-03T00:00:00Z")
}
{
  "_id" : ObjectId("58731fd2878544f6a1e470e0"),
  "date" : NumberLong("1483113600000"),
  "M0000612" : 2.3,
  "M0009973" : 7946,
  "M0001385" : 11.4,
  "M0010049" : 30105.17,
  "M0001383" : 21.4,
  "G1500013" : 55.6,
  "G0002299" : 54.9,
  "G0002322" : 52.4,
  "G0006318" : 56.1,
  "G1400007" : 53.5,
  "G0002323" : 54.7,
  "dateObj" : ISODate("2016-12-31T00:00:00Z")
}

> db.wind_edb.find().count()
5778

----------- 2017-02-06 11:21:15
to request for measures: 
['M0061614']
2017-02-05
2017-02-05
kidd@kidd-OptiPlex-9020:
~/workspace/abc/apes-crawler
$ {'errorCode': 0, 'codeList': ['M0061614'], 'data': [-700.0], 'fieldList': ['CLOSE'], 'timeList': ['2017-02-03T00:00:00']}


$ to request for measures: 
['M0061614', 'S2703477']
2017-02-05
2017-02-05
{'errorCode': 0, 'codeList': ['M0061614', 'S2703477'], 'data': ['NaN', 47.37, -700.0, 47.37], 'fieldList': ['CLOSE'], 'timeList': ['2017-01-23T00:00:00', '2017-02-03T00:00:00']}
1485100800000
1486051200000

[u'M0061614', u'S2703477', u'S2703476', u'S2703479', u'S2703478', u'S2703480', u'S2703481', u'S2707387', u'S2707388', u'M0062611']

fix:
  the results contain many lines with update-date as the line key, while for different indexes, their update-dates might be different. Hence skip all lines with NaN, the often the last line is the correct line with no NaN.
  this seems to be a big waste of quota

[root@bj-tz8-db001 ahye]# python test.py
2015-09-24T00:00:00
2015-10-24T00:00:00
2016-01-29T00:00:00
2017-01-24T00:00:00
2016-12-31T00:00:00 month, need to re-crawl
2017-02-04T00:00:00
got infos count: 6

------------- 2017-02-06 15:01:03
transform.py

# -*- coding:utf-8 -*- 

import json
import sys
from cprint import cprint

PHRASE_MAP = {
    '国家': 'nation',
    '表名': 'tableName',
    '指标名称': 'name',
    '频率': 'frequency',
    '单位': 'unit',
    '指标ID': 'id',
    '来源': 'source',
}

file = open("input.txt")
# file = open("/home/kidd/workspace/python/file/jy_raw_api")
output = open('output.txt', 'w')

try:
  raw_input = []
  while 1:
      # lines = file.readlines(100000)
      content = file.read()
      if not content:
          break
      lines = content.split('\n')
      height = len(lines)
      print str(len(lines)) + ' lines'
      # lines = content.split('\r\n')
      for i in range(len(lines)):
        arr = lines[i].split(' ')
        raw_input.append(arr)
        width = len(arr)
        # print "len: " + str(len(arr))
        if len(arr) <= 1:
          print 'invalid line detected'
          continue
        # print lines[i]
        # print str(i) + ': ' + str(len(arr) - 1)
        # if arr[2] == '股票' or arr[3] == '股票':
        #   output.write(arr[0] + ',' + arr[1] + ',' + arr[len(arr) - 1] + '\n')
        print 'lenght of line fields: ' + str(len(arr)) + ' starting with ' + PHRASE_MAP[arr[0]]
        # break
      
      docs = []
      for i in range(1, width):
        # print '----------' + str(height)
        doc = {}
        for j in range(0, height):
          # print PHRASE_MAP[raw_input[j][0]]
          key = '\'' + PHRASE_MAP[raw_input[j][0]] + '\''
          if PHRASE_MAP[raw_input[j][0]] == 'frequency':
            if raw_input[j][i] == '月':
              doc[key] = '\'month\''
            elif raw_input[j][i] == '日':
              doc[key] = '\'day\''
            elif raw_input[j][i] == '周':
              doc[key] = '\'week\''
            # elif raw_input[j][i] == '季':
            #   doc[key] = '\'quater\''
            else:
              doc[key] = '\'' + raw_input[j][i] + '\''
          else:
            doc[key] = '\'' + raw_input[j][i] + '\''
        # print json.dumps(doc)
        doc['\'type\''] = 2
        doc['\'dataType\''] = 4
        doc['\'datasetId\''] = '\'wind_edb\''
        docs.append(doc)
      cprint(docs)

finally:
  file.close()
  output.close()

---------- 2017-02-06 15:45:35
curl 'http://120.26.95.121/api/v1/chart/query-data?id=773&tqx=reqId%3A0' -H 'Accept-Encoding: gzip, deflate, sdch' -H 'Accept-Language: en-US,en;q=0.8,zh-CN;q=0.6,zh;q=0.4' -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36' -H 'Accept: */*' -H 'Referer: http://120.26.95.121/views/dashboard/index.html' -H 'X-DataSource-Auth: a' -H 'Cookie: token=eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJKZXJzZXktU2VjdXJpdHktQmFzaWMiLCJzdWIiOiI1ODNiOWYxOTVkMzJjYjJjYTNkOGM5ZTciLCJhdWQiOiJ1c2VyIiwiZXhwIjoxNDg2NTM4MjE0LCJpYXQiOjE0ODYzNjU0MTQsImp0aSI6IjEifQ.0ttXsgHRQe460WsCj8teup2s0HNI_cfK--fgKC7lWko' -H 'Connection: keep-alive' --compressed

cache method needs to be improved.

-------- 2017-02-06 16:55:12
> db.wind_edb.find({"G1100313" : {$exists : true}}, {'date' : 1}).sort({date : -1})
{ "_id" : ObjectId("583e8e8fc3f6af591d8413ef"), "date" : NumberLong("1480435200000") }
2016/11/30 0:0:0

after adding more measures for 'us treasures'
> db.measures.find({datasetId : 'wind_edb'}).count()
674

------------- 2017-02-07 15:25:16
data on 12-31 (1483113600000) missing in M0043804
> db.wind_edb.find({'M0043804' : {$exists : true } }, {date : 1}).sort({date : -1}).limit(6)
{ "_id" : ObjectId("589442b7c189e5c954bb7539"), "date" : NumberLong("1485792000000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413ef"), "date" : NumberLong("1480435200000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413df"), "date" : NumberLong("1477843200000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413d3"), "date" : NumberLong("1475164800000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413c3"), "date" : NumberLong("1472572800000") }
{ "_id" : ObjectId("583ec4c9c3f6af591d8414d6"), "date" : NumberLong("1469894400000") }
> db.wind_edb.find({'M0000612' : {$exists : true } }, {date : 1}).sort({date : -1}).limit(6)
{ "_id" : ObjectId("589442b7c189e5c954bb7539"), "date" : NumberLong("1485792000000") }
{ "_id" : ObjectId("58731fd2878544f6a1e470e0"), "date" : NumberLong("1483113600000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413ef"), "date" : NumberLong("1480435200000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413df"), "date" : NumberLong("1477843200000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413d3"), "date" : NumberLong("1475164800000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413c3"), "date" : NumberLong("1472572800000") }

db.wind_edb.find({'M0001227' : {$exists : true } }, {date : 1}).sort({date : -1}).limit(6)
{ "_id" : ObjectId("589442b7c189e5c954bb7539"), "date" : NumberLong("1485792000000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413ef"), "date" : NumberLong("1480435200000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413df"), "date" : NumberLong("1477843200000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413d3"), "date" : NumberLong("1475164800000") }
{ "_id" : ObjectId("583e8e8fc3f6af591d8413c3"), "date" : NumberLong("1472572800000") }
{ "_id" : ObjectId("583ec4c9c3f6af591d8414d6"), "date" : NumberLong("1469894400000") }



db.wind_wsd.find({windcode : '000001.SH'}, {date : 1}).sort({date : -1}).limit(100)
{ "_id" : ObjectId("58844dd8878544f6a1e51fe2"), "date" : NumberLong("1484841600000") }
{ "_id" : ObjectId("5879e9ee878544f6a1e51f55"), "date" : NumberLong("1484323200000") }
{ "_id" : ObjectId("5879e9ee878544f6a1e51f54"), "date" : NumberLong("1484236800000") }
{ "_id" : ObjectId("5878b8b9878544f6a1e473d4"), "date" : NumberLong("1484150400000") }
{ "_id" : ObjectId("5878b8b9878544f6a1e473d3"), "date" : NumberLong("1484064000000") }
{ "_id" : ObjectId("5878b8b8878544f6a1e473d2"), "date" : NumberLong("1483977600000") }
{ "_id" : ObjectId("5878b8b8878544f6a1e473d1"), "date" : NumberLong("1483891200000") }
{ "_id" : ObjectId("5878b8b8878544f6a1e473d0"), "date" : NumberLong("1483804800000") }
{ "_id" : ObjectId("5878b8b8878544f6a1e473cf"), "date" : NumberLong("1483718400000") }
....
{ "_id" : ObjectId("5878b8b8878544f6a1e473c9"), "date" : NumberLong("1483200000000"), "windcode" : "000001.SH", "close" : 3103.6373 }
{ "_id" : ObjectId("5878b8b8878544f6a1e473c8"), "date" : NumberLong("1483113600000"), "windcode" : "000001.SH", "close" : 3103.6373 }
{ "_id" : ObjectId("5878b8b8878544f6a1e473c7"), "date" : NumberLong("1483027200000"), "windcode" : "000001.SH", "close" : 3103.6373 }
{ "_id" : ObjectId("5878b8b8878544f6a1e473c6"), "date" : NumberLong("1482940800000"), "windcode" : "000001.SH", "close" : 3096.0968 }
{ "_id" : ObjectId("5878b8b8878544f6a1e473c5"), "date" : NumberLong("1482854400000"), "windcode" : "000001.SH", "close" : 3102.2357 }
{ "_id" : ObjectId("5878b8b8878544f6a1e473c4"), "date" : NumberLong("1482768000000"), "windcode" : "000001.SH", "close" : 3114.664 }


to crawl the data for the specific index:
  date_ranges = [[datetime.fromtimestamp(1483113600), datetime.fromtimestamp(1483113600)]]
  for start, stop, measures in self._get_measures(date_ranges):
      if start is None or stop is None or len(measures) == 0:
          self.logger.info('no measures need to be crawled')
          return
      measures = ['M0043804']

---------- 2017-02-09 15:29:07
echo "2017 1 14 2017 2 5" > /tmp/spider.ranges
export CRAWLER_ENV=release
sudo python runcrawler.py offline "Daily Index Market Spider"


online index data:
> db.wind_wsd.find({windcode : '000001.SH'}, {date : 1}).sort({date : -1}).limit(20).pretty()
{
  "_id" : ObjectId("589be5afca6d38f77b4f5cbe"),
  "date" : NumberLong("1486483200000")
}
{
  "_id" : ObjectId("5899ef14ca6d38f77b4f4503"),
  "date" : NumberLong("1486396800000")
}
{
  "_id" : ObjectId("58989d90ca6d38f77b4f2d14"),
  "date" : NumberLong("1486310400000")
}
{
  "_id" : ObjectId("5879f3b1b14e9901adf5ed97"),
  "date" : NumberLong("1484323200000") # 2017/1/14 0:0:0
}
......

temp fix: 
  add below param to "index_market" spider
    'extraParams': 'Days=Alldays;Fill=Previous',

> db.charts.find({_id : 761}, {thumbnail : 0}).pretty()
{
  "_id" : NumberLong(761),
  "chartLib" : "highChart",
  "title" : "通胀、利率、实际利率、上证指数",
  "dataSource" : {
    "parameters" : {
      ":endDate" : {
        "default" : "1478188800000"
      },
      ":startDate" : {
        "default" : "1477584000000"
      }
    },
    "columns" : [
      {
        "type" : "string",
        "dataType" : "date",
        "id" : "date",
        "label" : "日期",
        "pattern" : "yyyy-MM"
      },
      {
        "type" : "number",
        "id" : "M0000612",
        "label" : "CPI:当月同比"
      },
      {
        "type" : "number",
        "id" : "M0043804",
        "label" : "定期存款利率:1年(整存整取)(月)"
      },
      {
        "type" : "number",
        "id" : "s2",
        "label" : "上证指数(右)"
      },
      {
        "type" : "number",
        "id" : "s1",
        "label" : "实际利率(定存-CPI)"
      }
    ],
    "aggregates" : [
      {
        "collection" : "wind_edb",
        "aggregate" : [
          {
            "＄match" : {
              "M0000612" : {
                "＄exists" : true
              },
              "M0043804" : {
                "＄exists" : true
              }
            }
          },
          {
            "＄project" : {
              "_id" : 0,
              "date" : 1,
              "M0000612" : 1,
              "M0043804" : 1,
              "s1" : {
                "＄subtract" : [
                  "$M0043804",
                  "$M0000612"
                ]
              }
            }
          },
          {
            "＄lookup" : {
              "from" : "wind_wsd",
              "localField" : "date",
              "foreignField" : "date",
              "as" : "s2"
            }
          },
          {
            "＄unwind" : "$s2"
          },
          {
            "＄match" : {
              "s2．windcode" : "000001.SH"
            }
          },
          {
            "＄group" : {
              "_id" : "$date",
              "date" : {
                "＄first" : "$date"
              },
              "M0000612" : {
                "＄first" : "$M0000612"
              },
              "M0043804" : {
                "＄first" : "$M0043804"
              },
              "s1" : {
                "＄first" : "$s1"
              },
              "s2" : {
                "＄first" : "$s2.close"
              }
            }
          },
          {
            "＄sort" : {
              "date" : 1
            }
          }
        ]
      }
    ]
  },
  "options" : {
    "series" : [
      {
        "color" : "#984807"
      },
      {
        "color" : "#E46C0A"
      },
      {
        "color" : "#aaaaaa",
        "dashStyle" : "shortdash",
        "yAxis" : 1
      },
      {
        "color" : "#FAC090",
        "type" : "column"
      }
    ],
    "xAxis" : [
      {
        "tickInterval" : 5,
        "labels" : {
          "rotation" : -45
        },
        "type" : "category"
      }
    ],
    "yAxis" : [
      {
        "title" : {
          "text" : null
        },
        "max" : 30,
        "min" : -30,
        "tickInterval" : 15
      },
      {
        "title" : {
          "text" : null
        },
        "labels" : {
          "format" : "{value:,.0f}"
        },
        "opposite" : true,
        "max" : 6000,
        "tickInterval" : 1500
      }
    ],
    "chart" : {
      "marginBottom" : 110,
      "marginRight" : 50,
      "alignTicks" : false
    },
    "legend" : {
      "itemMarginTop" : 10
    },
    "plotOptions" : {
      "line" : {
        "marker" : {
          "enabled" : false
        }
      }
    }
  },
  "tags" : [ ],
  "status" : 0,
  "company_id" : "system",
  "creator_id" : "58398b6952fe904c135d47c8",
  "create_at" : "2017-01-22 18:02:44.648",
  "update_at" : "2017-02-08 11:42:24.311"
}

------------- 2017-02-09 19:15:46
  print 'get_market_measurements(): ' + str(len(get_market_measurements()))
  print 'get_analysis_measurements(): ' + str(len(get_analysis_measurements()))
  print 'edb measures: ' + str(len(Dataset.get_measures('wind_edb', id_only = False)))
  print 'wind_performance measures: ' + str(len(Dataset.get_measures('wind_performance', id_only = False)))
  print 'wind_finance measures: ' + str(len(Dataset.get_measures('wind_finance', id_only = False)))
  print 'wind_events measures: ' + str(len(Dataset.get_measures('wind_events', id_only = False)))
  print 'get_index_codes(): ' + str(len(get_index_codes()))
  print 'get_zj_index_codes(): ' + str(len(get_zj_index_codes()))
  print 'SectorManager.get_securities(u"沪深股票/市场类/全部A股"): ' + str(len(SectorManager.get_securities(u'沪深股票/市场类/全部A股')))

$ sudo python runcrawler.py offline "Daily Market Spider"
['anhye@abcft.com']
2017-02-09 19:15:29,736 - 16064 - INFO - connect websocket
2017-02-09 19:15:29,961 - 16064 - INFO - connect finished
get_market_measurements(): 105
get_analysis_measurements(): 144
edb measures: 674
wind_performance measures: 281
wind_finance measures: 1435
wind_events measures: 235

[105 market measurements]
get_index_codes(): 9
get_zj_index_codes(): 62
2017-02-09 19:15:30,063 - 16064 - INFO - close websocket
run offline spider "Daily Market Spider" finished


------------ 2017-03-02 11:58:28
[2017-03-02 00:12:13.289] [INFO] app - send to agent:  { cmd: 'wsd',
  type: 'cmd',
  param: 
   [ '000001.SZ',
     'pre_close,open,high,low,close,volume,amt,dealnum,chg,pct_chg,swing,vwap,adjfactor,close2,turn,free_turn,last_trade_day,rel_ipo_chg,rel_ipo_pct_chg,trade_status,susp_days,susp_reason,maxupordown,open3,high3,low3,close3,mrg_long_amt,mrg_long_repay,mrg_long_bal,mrg_short_vol,mrg_short_vol_repay,margin_saletradingamount,lastradeday_s,margin_salerepayamount,mrg_short_vol_bal,mrg_short_bal,shortsell_turnover,shortsell_turnoverpct,shortsell_volume,shortsell_volumepct,shortsell_volumetohshares,mfd_buyamt_d,mfd_sellamt_d,mfd_buyvol_d,mfd_sellvol_d,mfd_netbuyamt,mfd_netbuyvol,mfd_buyord,mfd_sellord,mfd_buyamt_a,mfd_sellamt_a,mfd_buyvol_a,mrg_bal,mfd_sellvol_a,mfd_netbuyamt_a,mfd_netbuyvol_a,mf_amt_open,mf_amt_close,mf_amt_ratio,mfd_inflowrate_open_a,mfd_inflowrate_close_a,mfd_inflowproportion_a,mfd_inflowproportion_open_a,mfd_inflowproportion_close_a,mf_vol,mfd_inflowvolume_open_a,mfd_inflowvolume_close_a,mfd_volinflowrate_a,mfd_volinflowrate_open_a,mfd_volinflowrate_close_a,mf_amt,mfd_volinflowproportion_open_a,mf_vol_ratio,mfd_inflow_m,mfd_inflow_open_m,mfd_inflow_close_m,mfd_inflowrate_m,mfd_inflowrate_open_m,mfd_inflowrate_close_m,mfd_volinflowproportion_close_a,mfd_inflowproportion_open_m,mfd_inflowproportion_close_m,mfd_buyvol_m,mfd_buyvol_open_m,mfd_buyvol_close_m,mfd_volinflowrate_m,mfd_volinflowrate_open_m,mfd_volinflowrate_close_m,mfd_volinflowproportion_m,mfd_volinflowproportion_open_m,mfd_volinflowproportion_close_m,us_preclose,us_open,us_high,us_low,us_close,us_avgprice,us_change,us_pctchange,us_turn,us_volume,us_amount,us_swing,mfd_inflowproportion_m',
     '2017-03-01',
     '2017-03-01' ],
  id: 6020 }
[2017-03-02 00:15:33.593] [ERROR] app - Error  { [Error: read ETIMEDOUT] code: 'ETIMEDOUT', errno: 'ETIMEDOUT', syscall: 'read' }
Error: read ETIMEDOUT
    at exports._errnoException (util.js:907:11)
    at TCP.onread (net.js:557:26)
[2017-03-02 00:15:33.644] [INFO] app - connection closed  9z1te5IhMM9LfsTjsVQ7Xw==
[2017-03-02 00:18:08.067] [INFO] app - connection closed  yckItH6CYzAvfL40vs+geg==
[2017-03-02 00:18:08.068] [INFO] app - connection closed  eX7CeIl+7ApiuZKhQ0SPQw==
[2017-03-02 00:18:08.070] [INFO] app - connection vaHKOR+DzAstE9yTjMojYg== from 10.117.84.60
[2017-03-02 00:18:08.071] [INFO] app - connection kBPLnSImQ2WpBnS1hYV31g== from 10.117.84.60
[2017-03-02 00:18:08.091] [INFO] app - cmd: {"cmd": "wsd", "type": "cmd", "param": ["000903.SH", "pre_close,open,high,low,close,volume,amt,dealnum,chg,pct_chg,swing,vwap,adjfactor,close2,turn,free_turn,last_trade_day,rel_ipo_chg,rel_ipo_pct_chg,trade_status,susp_days,susp_reason,maxupordown,open3,high3,low3,close3,mrg_long_amt,mrg_long_repay,mrg_long_bal,mrg_short_vol,mrg_short_vol_repay,margin_saletradingamount,lastradeday_s,margin_salerepayamount,mrg_short_vol_bal,mrg_short_bal,shortsell_turnover,shortsell_turnoverpct,shortsell_volume,shortsell_volumepct,shortsell_volumetohshares,mfd_buyamt_d,mfd_sellamt_d,mfd_buyvol_d,mfd_sellvol_d,mfd_netbuyamt,mfd_netbuyvol,mfd_buyord,mfd_sellord,mfd_buyamt_a,mfd_sellamt_a,mfd_buyvol_a,mrg_bal,mfd_sellvol_a,mfd_netbuyamt_a,mfd_netbuyvol_a,mf_amt_open,mf_amt_close,mf_amt_ratio,mfd_inflowrate_open_a,mfd_inflowrate_close_a,mfd_inflowproportion_a,mfd_inflowproportion_open_a,mfd_inflowproportion_close_a,mf_vol,mfd_inflowvolume_open_a,mfd_inflowvolume_close_a,mfd_volinflowrate_a,mfd_volinflowrate_open_a,mfd_volinflowrate_close_a,mf_amt,mfd_volinflowproportion_open_a,mf_vol_ratio,mfd_inflow_m,mfd_inflow_open_m,mfd_inflow_close_m,mfd_inflowrate_m,mfd_inflowrate_open_m,mfd_inflowrate_close_m,mfd_volinflowproportion_close_a,mfd_inflowproportion_open_m,mfd_inflowproportion_close_m,mfd_buyvol_m,mfd_buyvol_open_m,mfd_buyvol_close_m,mfd_volinflowrate_m,mfd_volinflowrate_open_m,mfd_volinflowrate_close_m,mfd_volinflowproportion_m,mfd_volinflowproportion_open_m,mfd_volinflowproportion_close_m,us_preclose,us_open,us_high,us_low,us_close,us_avgprice,us_change,us_pctchange,us_turn,us_volume,us_amount,us_swing,mfd_inflowproportion_m", "2017-03-01", "2017-03-01", "Days=Alldays;Fill=Previous"]}
[2017-03-02 00:18:08.092] [WARN] app - no client found

c# agent:
2017-03-02 11:56:32 wd started
2017-03-02 11:56:32 cmd come: wsd
2017-03-02 11:57:39 wd stopped
2017-03-02 12:57:29 Closed: An exception has occurred while receiving.
2017-03-02 12:57:30 Closed: An exception has occurred while connecting.
2017-03-02 12:57:31  Closed: An exception has occurred while connecting.
2017-03-02 12:57:32  Closed: An exception has occurred while connecting.
2017-03-02 12:57:33  Closed: An exception has occurred while connecting.
  https://github.com/sta/websocket-sharp/blob/master/websocket-sharp/WebSocket.cs
